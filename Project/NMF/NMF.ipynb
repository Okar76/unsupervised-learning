{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12f8aca-e5c0-4839-b18c-61b2628f261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6238fdbd-4970-4f0d-bab6-d8ca6192e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/documents.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9898d6-0783-42fe-8929-76f1c7c04a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom list of common words to remove (can be modified as needed)\n",
    "custom_stopwords = [\n",
    "    'science', 'understanding', 'continues', 'however', 'also', 'among',\n",
    "    'using', 'within', 'based', 'many', 'different', 'new', 'including',\n",
    "    'related', 'often', 'such', 'one', 'two', 'first', 'second'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa81a3a-b9d6-4f64-ad24-9b1e7c89b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paragraphs to numerical representation using TF-IDF with enhancements\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',    # Remove default English stop words\n",
    "    max_df=0.85,             # Ignore words appearing in more than 85% of paragraphs\n",
    "    min_df=2,                # Ignore words appearing in fewer than 2 paragraphs\n",
    "    max_features=None        # No limit on maximum number of words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ae7cc5-0bfb-437c-b785-ba69b3089da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge custom stop words list with the default stop words list\n",
    "vectorizer.stop_words_ = vectorizer.get_stop_words().union(custom_stopwords)\n",
    "\n",
    "X = vectorizer.fit_transform(paragraphs)\n",
    "\n",
    "# Determine the required number of topics\n",
    "n_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1658120b-ab62-4040-8de9-bf4dd157feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=n_topics, random_state=42, init='nndsvda', max_iter=500)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "371b6b8b-2be0-482d-a07e-755f4878beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the important topics\n",
    "def print_topics(H, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        print(\", \".join(top_features))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842a523e-1010-4cbd-8870-a90f833aa55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "practical, remain, significant, despite, advances, applications, challenges, continues, sustainable, chronological\n",
      "\n",
      "Topic 2:\n",
      "epidemiological, security, navigation, entanglement, augmentation, continues, enhancement, prosthetics, expansion, circuitry\n",
      "\n",
      "Topic 3:\n",
      "approaches, promise, traditionally, new, field, combining, dominated, singularities, continues, astrophysical\n",
      "\n",
      "Topic 4:\n",
      "biotechnological, dialects, preservation, ecosystems, synthesis, currencies, kinetics, approaches, continues, expansion\n",
      "\n",
      "Topic 5:\n",
      "renewable, nanotechnological, robotics, enhancement, security, energy, dialects, biodiversity, civilizations, realities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print_topics(H, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9452d-13a2-484d-8cf0-626b53a12570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c440a3b-e04a-4137-8ba8-d588fed7dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
